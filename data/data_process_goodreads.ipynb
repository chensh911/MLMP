{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b91ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CSH/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd1f4e",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b4fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'GoodReads'\n",
    "path = '/root/TKDE/data'\n",
    "is_vocabulary = True\n",
    "\n",
    "meta_data_path = {\n",
    "    \"patent\": \"abstract.csv\",\n",
    "    \"company\": \"Assignees-token.csv\",\n",
    "    \"applicant\": \"Applicants-token.csv\",\n",
    "    \"patent_filter\": \"patent-filter.csv\",\n",
    "    \"company_filter\": \"company-filter.csv\",\n",
    "    \"applicant_filter\":\"applicant-filter.csv\",\n",
    "}\n",
    "center_node_type = \"patent\"\n",
    "center_edge = ('patent', 'applicant')\n",
    "feat_name = {\n",
    "    \"applicant\": \"A\",\n",
    "    \"company\": \"C\",\n",
    "    \"patent\": \"P\"\n",
    "}\n",
    "\n",
    "edge_name = {\n",
    "    'applicant_patent': ['A', 'P'],\n",
    "    'company_patent': ['C', 'P'],\n",
    "    'patent_applicant': ['P', 'A'],\n",
    "    'patent_company': ['P', 'C'],\n",
    "}\n",
    "\n",
    "link_prediction_type = 'company_patent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d6b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for k in meta_data_path:\n",
    "    data_path = os.path.join(folder, meta_data_path[k])\n",
    "    data_dict[k] = pd.read_csv(data_path).dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1384604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = os.path.join(folder, 'model', 'bert_raw.pkl')\n",
    "dataset = dgl.load_graphs(graph_path)\n",
    "graph = dataset[0][0]\n",
    "etypes = {ntype: [] for ntype in graph.ntypes}\n",
    "embedding = dict()\n",
    "for ntype in graph.ntypes:\n",
    "    if \"feat\" in graph.nodes[ntype].data:\n",
    "        embedding[ntype] = graph.nodes[ntype].data[\"feat\"]\n",
    "\n",
    "node_num = {}\n",
    "nodeEmb = nn.ModuleDict()\n",
    "for ntype in graph.ntypes:\n",
    "    node_num[ntype] = len(graph.nodes(ntype))\n",
    "    nor_feat = graph.nodes[ntype].data[\"feat\"]\n",
    "\n",
    "    nodeEmb[ntype] = nn.Embedding.from_pretrained(\n",
    "        torch.cat([nor_feat, torch.zeros(1, nor_feat.shape[1])], dim=0), freeze=True)\n",
    "\n",
    "for stype, etype, dtype in graph.canonical_etypes:\n",
    "    etypes[stype].append((etype, dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025bfec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[1]\n",
    "dgl.save_graphs(os.path.join(folder, 'model', 'bert_pretrain.pkl') , graph, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9bea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for x in data_dict[center_node_type].values[:, -1]:\n",
    "    text = str(x).replace(\" \", \"\")[1:-1].split(\",\")[1:]\n",
    "    texts.append(' '.join(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'applicant': 154418, 'company': 40135, 'patent': 364115},\n",
       "      num_edges={('applicant', 'applicant_patent', 'patent'): 572654, ('company', 'company_patent', 'patent'): 466626, ('patent', 'patent_applicant', 'applicant'): 572654, ('patent', 'patent_company', 'company'): 466626},\n",
       "      metagraph=[('applicant', 'patent', 'applicant_patent'), ('patent', 'applicant', 'patent_applicant'), ('patent', 'company', 'patent_company'), ('company', 'patent', 'company_patent')])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "92902e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 2\n",
    "return_dict = [{ntype: [] for ntype in etypes.keys()} for i in range(3)]\n",
    "def sampleNei(ntype: dict, etypes: list, nei_num: int) -> dict:\n",
    "        neigh_layer_1 = dgl.sampling.sample_neighbors(graph, ntype, nei_num, edge_dir='out')\n",
    "        neigh_nodes = {dtype: set() for etype, dtype in etypes}\n",
    "        for etype_tuple in etypes:\n",
    "            etype, dsttype = etype_tuple\n",
    "            neigh_nodes[dsttype].update(neigh_layer_1.edges(etype=etype)[1].unique().numpy().tolist())\n",
    "        for ntype, nodes in neigh_nodes.items():\n",
    "            neigh_nodes[ntype] = nodes\n",
    "        return neigh_nodes\n",
    "layer_num = 2\n",
    "nei_num = 5\n",
    "graph_neighbours = {}\n",
    "for nid in graph.nodes(center_node_type).numpy().tolist():\n",
    "    graph_neighbour = {}\n",
    "    preNeis = {center_node_type: set([nid])}\n",
    "    applicant_nodes = list(sampleNei({center_node_type: [nid]}, [('patent_applicant', 'applicant')], 3)['applicant'])\n",
    "    neighbour_nodes = []\n",
    "    for applicant_node in applicant_nodes:\n",
    "        tmp_neighbour_nodes = list(sampleNei({'applicant': [applicant_node]}, [('applicant_patent', 'patent')], nei_num)['patent'])\n",
    "        neighbour_nodes += tmp_neighbour_nodes\n",
    "    neighbour_nodes = set(neighbour_nodes)\n",
    "    if nid in neighbour_nodes:\n",
    "         neighbour_nodes.remove(nid)\n",
    "    text_neighbour_id = list(neighbour_nodes)\n",
    "    \n",
    "    graph_neighbours[nid] = text_neighbour_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d70e71d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502578/1905231611.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_split_idx = {'train':{center_node_type: torch.tensor(permute_idx[:train_len])},\n",
      "/tmp/ipykernel_502578/1905231611.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'valid':{center_node_type: torch.tensor(permute_idx[train_len:node_len-test_len])},\n",
      "/tmp/ipykernel_502578/1905231611.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'test':{center_node_type: torch.tensor(permute_idx[node_len-test_len:])}}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed=2024)\n",
    "train_rate, test_rate = 0.7, 0.1\n",
    "node_len = graph.number_of_nodes(center_node_type)\n",
    "permute_idx = torch.randperm(node_len)\n",
    "train_len, test_len = int(node_len*train_rate), int(node_len*test_rate)\n",
    "node_split_idx = {'train':{center_node_type: torch.tensor(permute_idx[:train_len])}, \n",
    "                  'valid':{center_node_type: torch.tensor(permute_idx[train_len:node_len-test_len])},\n",
    "                  'test':{center_node_type: torch.tensor(permute_idx[node_len-test_len:])}}\n",
    "\n",
    "edge_split_idx = {}\n",
    "\n",
    "for k in node_split_idx:\n",
    "    edge_idx_lst = []\n",
    "    for node_id in node_split_idx[k][center_node_type]:\n",
    "        edge_idx = graph.in_edges(node_id, etype=link_prediction_type,form='eid')\n",
    "        if len(edge_idx) != 0:\n",
    "            edge_idx_lst.append(int(edge_idx[0]))\n",
    "    edge_split_idx[k] = {center_node_type: torch.tensor(edge_idx_lst)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690c924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(node_split_idx, os.path.join(folder, 'downstream', 'node_split_idx.pkl'))\n",
    "torch.save(edge_split_idx, os.path.join(folder, 'downstream', 'edge_split_idx.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24eeaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = graph.clone()\n",
    "\n",
    "# 获得所有边类型\n",
    "edge_types = g.etypes\n",
    "filtered_edge_types = [etype for etype in edge_types if etype != 'rev_cites']\n",
    "g = g.edge_type_subgraph(filtered_edge_types)\n",
    "\n",
    "# 遍历节点数据字典，创建新的键并复制值\n",
    "for k in feat_name:\n",
    "    tmp = g.nodes[k].data['feat']\n",
    "    g.nodes[k].data.clear()\n",
    "    g.nodes[k].data[feat_name[k]] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6584a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edges = {}\n",
    "edge_types = g.etypes\n",
    "for edge_type in edge_types:\n",
    "    adj = g.edges(etype=edge_type)\n",
    "    src_type, dst_type = edge_name[edge_type]\n",
    "    new_edges[(src_type, src_type+'-'+dst_type, dst_type)] = adj\n",
    "\n",
    "new_graph = dgl.heterograph(new_edges)\n",
    "\n",
    "for k in feat_name:\n",
    "    v = feat_name[k]\n",
    "    new_graph.nodes[v].data[v] = g.nodes[k].data[v]\n",
    "\n",
    "g = new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbcb8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hg_propagate_feat_dgl(g, tgt_type, num_hops, max_length, extra_metapath, echo=False):\n",
    "    for hop in range(1, max_length):\n",
    "        reserve_heads = [ele[:hop] for ele in extra_metapath if len(ele) > hop]\n",
    "        for etype in g.etypes:\n",
    "            stype, _, dtype = g.to_canonical_etype(etype)\n",
    "            # if hop == args.num_hops and dtype != tgt_type: continue\n",
    "            for k in list(g.nodes[stype].data.keys()):\n",
    "                if len(k) == hop:\n",
    "                    current_dst_name = f'{dtype}{k}'\n",
    "                    if (hop == num_hops and dtype != tgt_type and k not in reserve_heads) \\\n",
    "                      or (hop > num_hops and k not in reserve_heads):\n",
    "                        continue\n",
    "                    if echo: print(k, etype, current_dst_name)\n",
    "                    g[etype].update_all(\n",
    "                        fn.copy_u(k, 'm'),\n",
    "                        fn.mean('m', current_dst_name), etype=etype)\n",
    "\n",
    "        # remove no-use items\n",
    "        for ntype in g.ntypes:\n",
    "            if ntype == tgt_type: continue\n",
    "            removes = []\n",
    "            for k in g.nodes[ntype].data.keys():\n",
    "                if len(k) <= hop:\n",
    "                    removes.append(k)\n",
    "            for k in removes:\n",
    "                g.nodes[ntype].data.pop(k)\n",
    "            if echo and len(removes): print('remove', removes)\n",
    "        gc.collect()\n",
    "\n",
    "        if echo: print(f'-- hop={hop} ---')\n",
    "        # for ntype in g.ntypes:\n",
    "        #     for k, v in g.nodes[ntype].data.items():\n",
    "        #         print(f'{ntype} {k} {v.shape}', v[:,-1].max(), v[:,-1].mean())\n",
    "        # if echo: print(f'------\\n')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d706c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A A-P PA\n",
      "C C-P PC\n",
      "P P-A AP\n",
      "P P-C CP\n",
      "remove ['A']\n",
      "remove ['C']\n",
      "remove ['P']\n",
      "-- hop=1 ---\n",
      "AP A-P PAP\n",
      "CP C-P PCP\n",
      "PA P-A APA\n",
      "PC P-A APC\n",
      "PA P-C CPA\n",
      "PC P-C CPC\n",
      "remove ['AP']\n",
      "remove ['CP']\n",
      "remove ['PA', 'PC']\n",
      "-- hop=2 ---\n"
     ]
    }
   ],
   "source": [
    "extra_metapath = []\n",
    "num_hops = 3\n",
    "g = hg_propagate_feat_dgl(g, center_node_type, num_hops, num_hops, extra_metapath, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21bcd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_type = feat_name[center_node_type]\n",
    "raw_feats = {}\n",
    "keys = list(g.nodes[tgt_type].data.keys())\n",
    "\n",
    "for k in keys:\n",
    "    raw_feats[k] = g.nodes[tgt_type].data.pop(k)\n",
    "feats = {k: v.detach().clone() for k, v in raw_feats.items()}\n",
    "feats_path = os.path.join(folder, 'feats_3.npy')\n",
    "np.save(feats_path, feats) # 注意带上后缀名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04454a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.load(feats_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf2ffa",
   "metadata": {},
   "source": [
    "# Generate Pretrain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff36b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_neighbours\n",
    "graph_neighbours_path = os.path.join(folder, 'train_pp_graph_neighbour.npy')\n",
    "np.save(graph_neighbours_path, graph_neighbours) # 注意带上后缀名\n",
    " \n",
    "# graph_neighbours_path = os.path.join(folder, 'train_pp_graph_neighbour.npy')\n",
    "# load_dict = np.load(graph_neighbours_path, allow_pickle=True).item()\n",
    "# graph_neighbours = load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6096e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_neighbours_path = os.path.join('/root/Base/data/GoodReads/train_pp_graph_neighbour.npy')\n",
    "load_dict = np.load(graph_neighbours_path, allow_pickle=True).item()\n",
    "graph_neighbours = load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in texts:\n",
    "    if '\\n' in t:\n",
    "        print(\"YES\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0df9218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_for_pp(file, mode='train'):\n",
    "    '''\n",
    "    paper: P, field_of_study: F, author: A\n",
    "    Each line is in this format:\n",
    "    query_P \\t q_n_P \\t q_n_A \\t q_n_F \\$\\$ key_P \\t k_n_P \\t k_n_A \\t k_n_F \n",
    "    '''\n",
    "    input_id = node_split_idx[mode][center_node_type]\n",
    "    no_paper_neighbour = 0\n",
    "    false_text = 0\n",
    "    all_id_set = set(graph_neighbours.keys())\n",
    "    with open(file,'w') as fout:\n",
    "        for idd in input_id:\n",
    "            idd = int(idd)\n",
    "            # sample query node & neighbour\n",
    "            query_n_paper = graph_neighbours[idd][center_node_type]\n",
    "            \n",
    "            ## generate key paper\n",
    "            n = list(set(query_n_paper) & all_id_set)\n",
    "            if len(query_n_paper) != 0 and len(n) != 0:\n",
    "                n = n[0]\n",
    "            else:\n",
    "                n = idd\n",
    "\n",
    "            # generate query text\n",
    "            query_text = texts[idd]\n",
    "            # generate key text\n",
    "            key_text = texts[n]\n",
    "            ## summary\n",
    "            id_lst = [str(idd), str(n)]\n",
    "            write_down = '\\t'.join(id_lst) +'\\$\\$'+ query_text+'\\$\\$'+key_text+'\\n'\n",
    "            a = write_down.strip().split('\\$\\$')\n",
    "            if a[1] == query_text and a[2] == key_text:\n",
    "                if '\\n' not in query_text and '\\n' not in key_text:\n",
    "                    fout.write(write_down)\n",
    "                else:\n",
    "                    false_text += 1\n",
    "            else:\n",
    "                false_text += 1\n",
    "\n",
    "    print(f'Finish writing data into {file}')\n",
    "    print(f'No neighbour paper:{no_paper_neighbour}, all:{len(input_id)}')\n",
    "    print(f'false_text:{false_text}')\n",
    "    print('****************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba4bd0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish writing data into GoodReads/train_pp.tsv\n",
      "No neighbour paper:0, all:254880\n",
      "false_text:0\n",
      "****************************\n",
      "Finish writing data into GoodReads/valid_pp.tsv\n",
      "No neighbour paper:0, all:72824\n",
      "false_text:0\n",
      "****************************\n",
      "Finish writing data into GoodReads/test_pp.tsv\n",
      "No neighbour paper:0, all:36411\n",
      "false_text:0\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "write_for_pp(file=os.path.join(folder, 'train_pp.tsv'), mode='train')\n",
    "write_for_pp(file=os.path.join(folder, 'valid_pp.tsv'), mode='valid')\n",
    "write_for_pp(file=os.path.join(folder, 'test_pp.tsv'), mode='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46f54008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_for_downstream_pp(file, mode='train'):\n",
    "    '''\n",
    "    Each line is in this format:\n",
    "    query_paper \\t q_n_paper1 \\t...\\t q_paperK \\t q_n_author1 \\t...\\t q_authorM \\t q_venue \\$\\$ key_paper \\t k_n_paper1 \\t...\\t k_n_paperK \\t k_n_author1 \\t...\\t k_aM \\t k_venue \n",
    "    K = paper_neighbour_num\n",
    "    M = author_neighbour_num\n",
    "    '''\n",
    "    no_paper_neighbour = 0\n",
    "    false_text = 0\n",
    "    with open(file,'w') as fout:\n",
    "        for idd in graph.nodes(center_node_type).numpy().tolist():\n",
    "            id_lst = [str(idd)]\n",
    "            query_text = texts[idd]\n",
    "\n",
    "            ## summary\n",
    "            write_down = '\\t'.join(id_lst) +'\\$\\$'+ query_text+'\\n'\n",
    "            a = write_down.strip().split('\\$\\$')\n",
    "            if a[1] == query_text:\n",
    "                if '\\n' not in query_text:\n",
    "                    fout.write(write_down)\n",
    "                else:\n",
    "                    false_text += 1\n",
    "            else:\n",
    "                false_text += 1\n",
    "    print(f'Finish writing data into {file}')\n",
    "    print(f'No neighbour paper:{no_paper_neighbour}, all:{len(graph.nodes(center_node_type))}')\n",
    "    print(f'false_text:{false_text}')\n",
    "    print('****************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "843b07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish writing data into GoodReads/infer_pp.tsv\n",
      "No neighbour paper:0, all:364115\n",
      "false_text:0\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "write_for_downstream_pp(file=os.path.join(folder, 'infer_pp.tsv'), mode='infer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
